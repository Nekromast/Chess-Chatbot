{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym_chess import ChessEnvV2\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from tqdm.notebook import tqdm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Play Game Example etwas mit Gawron's Code gemerged\n",
    "def play_game_AI_against_random(episodes=2, steps=50):\n",
    "    env = gym.make(\"ChessVsRandomBot-v2\")\n",
    "    total_rewards = 0\n",
    "    average_rewards = 0\n",
    "    steps_needed = 0\n",
    "\n",
    "    observation = env.reset()\n",
    "    policy.init_game(observation)\n",
    "\n",
    "\n",
    "    for i in range(episodes):\n",
    "        print(\"\\n\", \"=\" * 10, \"NEW GAME\", \"=\" * 10)\n",
    "        #env.render()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for j in range(steps):\n",
    "\n",
    "            #moves = env.possible_moves\n",
    "            #move = random.choice(moves)\n",
    "            move = policy(observation, env.possible_moves)\n",
    "\n",
    "            action = env.move_to_action(move)\n",
    "\n",
    "            # Eigene Aktion an das Spiel weitergeben\n",
    "            observation, step_reward, done, _ = env.step(action)\n",
    "            episode_reward += step_reward\n",
    "            print()\n",
    "\n",
    "            if done:\n",
    "                print(\">\" * 5, \"GAME\", i, \"REWARD:\", episode_reward)\n",
    "                #steps_needed = j\n",
    "                break\n",
    "\n",
    "        observation = env.reset()\n",
    "        policy.init_game(observation)\n",
    "\n",
    "\n",
    "\n",
    "        # Episode zu Ende\n",
    "        #total_rewards += episode_reward\n",
    "        average_rewards = 0.05 * episode_reward + (1- 0.05) * average_rewards\n",
    "        # Kein Reward Threshold angegeben\n",
    "        '''if average_rewards > env.spec.reward_threshold:\n",
    "            print(f\"Du bist zu gut für das Spiel :( \\n \"\n",
    "                  f\"Dein Average Reward beträgt {average_rewards} und du brauchtest nur {steps_needed} Schritte zum Sieg!\")\n",
    "        '''\n",
    "\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"#\" * 40)\n",
    "    print(\"#\" * 40)\n",
    "    print(\"#\" * 40)\n",
    "    print(\"\\nAVERAGE SCORE: \", average_rewards)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ========== NEW GAME ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Studium_local\\Chess Chatbot github\\venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:31: UserWarning: \u001B[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (8, 8)\u001B[0m\n",
      "  logger.warn(\n",
      "H:\\Studium_local\\Chess Chatbot github\\venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:174: UserWarning: \u001B[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001B[0m\n",
      "  logger.warn(\n",
      "H:\\Studium_local\\Chess Chatbot github\\venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:187: UserWarning: \u001B[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001B[0m\n",
      "  logger.warn(\n",
      "H:\\Studium_local\\Chess Chatbot github\\venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: \u001B[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'dict'>`\u001B[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of dict",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[29], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mplay_game_AI_against_random\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m200\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[15], line 21\u001B[0m, in \u001B[0;36mplay_game_AI_against_random\u001B[1;34m(episodes, steps)\u001B[0m\n\u001B[0;32m     15\u001B[0m episode_reward \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(steps):\n\u001B[0;32m     18\u001B[0m \n\u001B[0;32m     19\u001B[0m     \u001B[38;5;66;03m#moves = env.possible_moves\u001B[39;00m\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;66;03m#move = random.choice(moves)\u001B[39;00m\n\u001B[1;32m---> 21\u001B[0m     move \u001B[38;5;241m=\u001B[39m \u001B[43mpolicy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobservation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpossible_moves\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     23\u001B[0m     action \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mmove_to_action(move)\n\u001B[0;32m     25\u001B[0m     \u001B[38;5;66;03m# Eigene Aktion an das Spiel weitergeben\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[10], line 18\u001B[0m, in \u001B[0;36mACPolicy.__call__\u001B[1;34m(self, observation, moves)\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, observation, moves):\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mACTIONS \u001B[38;5;241m=\u001B[39m moves\n\u001B[1;32m---> 18\u001B[0m     probs, value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnet(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobservation\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     19\u001B[0m     m \u001B[38;5;241m=\u001B[39m Categorical(probs)\n\u001B[0;32m     20\u001B[0m     action \u001B[38;5;241m=\u001B[39m m\u001B[38;5;241m.\u001B[39msample()\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Could not infer dtype of dict"
     ]
    }
   ],
   "source": [
    "play_game_AI_against_random(5, 200)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'board': [[-3, -5, -4, -2, -1, -4, -5, -3], [-6, -6, -6, -6, -6, -6, -6, -6], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [6, 6, 6, 6, 6, 6, 6, 6], [3, 5, 4, 2, 1, 4, 5, 3]], 'current_player': 'WHITE', 'white_king_castle_is_possible': True, 'white_queen_castle_is_possible': True, 'black_king_castle_is_possible': True, 'black_queen_castle_is_possible': True, 'white_king_is_checked': False, 'black_king_is_checked': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Studium_local\\Chess Chatbot github\\venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:31: UserWarning: \u001B[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (8, 8)\u001B[0m\n",
      "  logger.warn(\n",
      "H:\\Studium_local\\Chess Chatbot github\\venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:174: UserWarning: \u001B[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001B[0m\n",
      "  logger.warn(\n",
      "H:\\Studium_local\\Chess Chatbot github\\venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:187: UserWarning: \u001B[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001B[0m\n",
      "  logger.warn(\n",
      "H:\\Studium_local\\Chess Chatbot github\\venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: \u001B[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'dict'>`\u001B[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"ChessVsRandomBot-v2\")\n",
    "print(env.reset())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Der Code von Gawron\n",
    "\n",
    "def render(env, img):\n",
    "    img.set_data(env.render())\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "def play_game_gawron(policy, episodes=2000, do_render = False, seed=100):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if do_render:\n",
    "        env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "    else:\n",
    "        env = gym.make(\"CartPole-v1\")\n",
    "    observation, info = env.reset(seed=seed)\n",
    "    policy.init_game(observation)\n",
    "\n",
    "    if do_render:\n",
    "        plt.ion()\n",
    "        plt.axis('off')\n",
    "        img = plt.imshow(env.render())\n",
    "\n",
    "    status = {}\n",
    "    episode = 0\n",
    "    status['steps'] = 0\n",
    "    status['episode_reward'] = 0\n",
    "    status['average_reward'] = 0\n",
    "    total_reward = 0\n",
    "\n",
    "\n",
    "    with tqdm(total=episodes) as pbar:\n",
    "        pbar.set_postfix(status)\n",
    "        while True:\n",
    "            try:\n",
    "                action = policy(observation)\n",
    "                observation, reward, terminated, truncated, info = env.step(action)\n",
    "                status['steps'] += 1\n",
    "                status['episode_reward'] += reward\n",
    "                if do_render:\n",
    "                    render(env, img)\n",
    "                policy.update(observation, reward, terminated, truncated, info, pbar)\n",
    "\n",
    "                if terminated or status['steps'] > 1000:\n",
    "                    episode += 1\n",
    "                    if episode > pbar.total:\n",
    "                        break\n",
    "                    total_reward += status['episode_reward']\n",
    "                    status['average_reward'] = 0.05 * status['episode_reward'] + (1 - 0.05) * status['average_reward']\n",
    "                    if status['average_reward'] > env.spec.reward_threshold:\n",
    "                        print(f\"Solved! Running reward is now {status['average_reward']} and \"\n",
    "                              f\"the last episode runs to {status['steps']} time steps!\")\n",
    "                        break\n",
    "\n",
    "                    pbar.set_postfix(status, refresh=episode % 10 == 0)\n",
    "                    pbar.update()\n",
    "                    status['steps'] = 0\n",
    "\n",
    "                    status['episode_reward'] = 0\n",
    "                    observation, info = env.reset()\n",
    "                    policy.init_game(observation)\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                break\n",
    "    env.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0022675   0.01702591 -0.03774482  0.01587995]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "observation, info = env.reset()\n",
    "print(observation)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class ACNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size=32, max_possible_moves=4):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(max_possible_moves, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size*4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(hidden_size*4, hidden_size),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        p = self.policy(x)\n",
    "        v = self.critic(x)\n",
    "        return p, v"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class ActorCriticBoardNetwork(nn.Module):\n",
    "    def __init__(self, obs_size, action_size):\n",
    "        super().__init__()\n",
    "        self.obs_size = obs_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Define the layers of your custom policy network\n",
    "        self.fc1 = nn.Linear(obs_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.actor = nn.Linear(32, action_size)\n",
    "        self.critic = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # Convert the observation into a numerical format\n",
    "        board = np.array(obs['board'])\n",
    "        one_hot_board = np.zeros((8, 8, 13))\n",
    "        for i in range(8):\n",
    "            for j in range(8):\n",
    "                piece = board[i][j]\n",
    "                if piece != 0:\n",
    "                    piece_idx = abs(piece) - 1\n",
    "                    one_hot_board[i][j][piece_idx] = 1\n",
    "                    if piece < 0:\n",
    "                        one_hot_board[i][j][12] = 1\n",
    "        flattened_board = one_hot_board.flatten()\n",
    "        obs_tensor = torch.FloatTensor(flattened_board)\n",
    "\n",
    "        # Feed the observation into the policy network\n",
    "        x = F.relu(self.fc1(obs_tensor))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        action_probs = F.softmax(self.actor(x), dim=-1)\n",
    "        state_value = self.critic(x)\n",
    "\n",
    "        return action_probs, state_value"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n",
    "\n",
    "class ACPolicy:\n",
    "\n",
    "    def __init__(self, gamma=0.99, lr=5e-3):\n",
    "        self.net = ActorCriticBoardNetwork()\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=lr)\n",
    "        self.mean_reward = None\n",
    "        self.games = 0\n",
    "        self.gamma = gamma\n",
    "        self.eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "    def __call__(self, observation, moves):\n",
    "\n",
    "        self.ACTIONS = moves\n",
    "\n",
    "        probs, value = self.net(torch.tensor(observation))\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "\n",
    "        self.memory.append(SavedAction(m.log_prob(action), value))\n",
    "        self.last_observation = observation\n",
    "\n",
    "        return self.ACTIONS[action.item()]\n",
    "\n",
    "    def init_game(self, observation):\n",
    "        self.memory = []\n",
    "        self.rewards = []\n",
    "        self.total_reward = 0\n",
    "\n",
    "    def update(self, observation, reward, terminated, truncated, info, status):\n",
    "        self.total_reward += reward\n",
    "        self.rewards.append(reward)\n",
    "        if terminated:\n",
    "            self.games += 1\n",
    "            if self.mean_reward is None:\n",
    "                self.mean_reward = self.total_reward\n",
    "            else:\n",
    "                self.mean_reward = self.mean_reward * 0.95 + self.total_reward * (1.0 - 0.95)\n",
    "\n",
    "            # calculate discounted reward and make it normal distributed\n",
    "            discounted = []\n",
    "            R = 0\n",
    "            for r in self.rewards[::-1]:\n",
    "                R = r + self.gamma * R\n",
    "                discounted.insert(0, R)\n",
    "            discounted = torch.tensor(discounted)\n",
    "            # discounted = (discounted - discounted.mean()) / (discounted.std() + self.eps)\n",
    "\n",
    "            policy_losses = []\n",
    "            value_losses = []\n",
    "            for mem, discounted_reward in zip(self.memory, discounted):\n",
    "                advantage = discounted_reward - mem.value.item()\n",
    "                policy_losses.append(-(mem.log_prob * advantage))\n",
    "\n",
    "                value_losses.append(F.smooth_l1_loss(mem.value, discounted_reward.unsqueeze(0)))\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if self.games % 1000 == 0:\n",
    "                self.save(f\"model_{self.games}.pt\")\n",
    "\n",
    "    def load(self, PATH):\n",
    "        checkpoint = torch.load(PATH)\n",
    "        self.net.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.games = checkpoint['games']\n",
    "        self.mean_reward = checkpoint['mean_reward']\n",
    "\n",
    "    def save(self, PATH):\n",
    "        torch.save({\n",
    "            'games': self.games,\n",
    "            'model_state_dict': self.net.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'mean_reward': self.mean_reward}, PATH)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'play_game_gawron' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m policy \u001B[38;5;241m=\u001B[39m ACPolicy()\n\u001B[1;32m----> 2\u001B[0m \u001B[43mplay_game_gawron\u001B[49m(policy, episodes\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, do_render\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'play_game_gawron' is not defined"
     ]
    }
   ],
   "source": [
    "policy = ACPolicy()\n",
    "play_game_gawron(policy, episodes=10, do_render=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
