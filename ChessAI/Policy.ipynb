{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym_chess import ChessEnvV2\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from gym import wrappers\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from tqdm.notebook import tqdm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "# Play Game Example etwas mit Gawron's Code gemerged\n",
    "def play_game_AI_against_random(episodes=2, steps=50):\n",
    "    env = gym.make(\"ChessVsRandomBot-v2\")\n",
    "    total_rewards = 0\n",
    "    average_rewards = 0\n",
    "    steps_needed = 0\n",
    "    for i in range(episodes):\n",
    "        initial_observation = env.reset()\n",
    "        policy.initgame(initial_observation)\n",
    "        print(\"\\n\", \"=\" * 10, \"NEW GAME\", \"=\" * 10)\n",
    "        #env.render()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for j in range(steps):\n",
    "\n",
    "            #moves = env.possible_moves\n",
    "            #move = random.choice(moves)\n",
    "            move = policy(observation, env.possible_moves)\n",
    "\n",
    "            action = env.move_to_action(move)\n",
    "\n",
    "            # Eigene Aktion an das Spiel weitergeben\n",
    "            observation, step_reward, done, _ = env.step(action)\n",
    "            episode_reward += step_reward\n",
    "            print()\n",
    "\n",
    "            if done:\n",
    "                print(\">\" * 5, \"GAME\", i, \"REWARD:\", episode_reward)\n",
    "                #steps_needed = j\n",
    "                break\n",
    "\n",
    "        # Episode zu Ende\n",
    "        #total_rewards += episode_reward\n",
    "        average_rewards = 0.05 * episode_reward + (1- 0.05) * average_rewards\n",
    "        # Kein Reward Threshold angegeben\n",
    "        '''if average_rewards > env.spec.reward_threshold:\n",
    "            print(f\"Du bist zu gut für das Spiel :( \\n \"\n",
    "                  f\"Dein Average Reward beträgt {average_rewards} und du brauchtest nur {steps_needed} Schritte zum Sieg!\")\n",
    "        '''\n",
    "\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"#\" * 40)\n",
    "    print(\"#\" * 40)\n",
    "    print(\"#\" * 40)\n",
    "    print(\"\\nAVERAGE SCORE: \", average_rewards)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "play_game_AI_against_random(5, 200)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [],
   "source": [
    "# Der Code von Gawron\n",
    "\n",
    "def render(env, img):\n",
    "    img.set_data(env.render())\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "def play_game_gawron(policy, episodes=2000, do_render = False, seed=100):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if do_render:\n",
    "        env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "    else:\n",
    "        env = gym.make(\"CartPole-v1\")\n",
    "    observation, info = env.reset(seed=seed)\n",
    "    policy.init_game(observation)\n",
    "\n",
    "    if do_render:\n",
    "        plt.ion()\n",
    "        plt.axis('off')\n",
    "        img = plt.imshow(env.render())\n",
    "\n",
    "    status = {}\n",
    "    episode = 0\n",
    "    status['steps'] = 0\n",
    "    status['episode_reward'] = 0\n",
    "    status['average_reward'] = 0\n",
    "    total_reward = 0\n",
    "\n",
    "\n",
    "    with tqdm(total=episodes) as pbar:\n",
    "        pbar.set_postfix(status)\n",
    "        while True:\n",
    "            try:\n",
    "                action = policy(observation)\n",
    "                observation, reward, terminated, truncated, info = env.step(action)\n",
    "                status['steps'] += 1\n",
    "                status['episode_reward'] += reward\n",
    "                if do_render:\n",
    "                    render(env, img)\n",
    "                policy.update(observation, reward, terminated, truncated, info, pbar)\n",
    "\n",
    "                if terminated or status['steps'] > 1000:\n",
    "                    episode += 1\n",
    "                    if episode > pbar.total:\n",
    "                        break\n",
    "                    total_reward += status['episode_reward']\n",
    "                    status['average_reward'] = 0.05 * status['episode_reward'] + (1 - 0.05) * status['average_reward']\n",
    "                    if status['average_reward'] > env.spec.reward_threshold:\n",
    "                        print(f\"Solved! Running reward is now {status['average_reward']} and \"\n",
    "                              f\"the last episode runs to {status['steps']} time steps!\")\n",
    "                        break\n",
    "\n",
    "                    pbar.set_postfix(status, refresh=episode % 10 == 0)\n",
    "                    pbar.update()\n",
    "                    status['steps'] = 0\n",
    "\n",
    "                    status['episode_reward'] = 0\n",
    "                    observation, info = env.reset()\n",
    "                    policy.init_game(observation)\n",
    "\n",
    "            except KeyboardInterrupt:\n",
    "                break\n",
    "    env.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class ACNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size=32, max_possible_moves=4):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(max_possible_moves, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size*4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(hidden_size*4, hidden_size),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        p = self.policy(x)\n",
    "        v = self.critic(x)\n",
    "        return p, v"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n",
    "\n",
    "class ACPolicy:\n",
    "\n",
    "    def __init__(self, gamma=0.99, lr=5e-3):\n",
    "        self.net = ACNetwork()\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=lr)\n",
    "        self.mean_reward = None\n",
    "        self.games = 0\n",
    "        self.gamma = gamma\n",
    "        self.eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "    def __call__(self, observation, moves):\n",
    "\n",
    "        self.ACTIONS = moves\n",
    "\n",
    "        probs, value = self.net(torch.tensor(observation))\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "\n",
    "        self.memory.append(SavedAction(m.log_prob(action), value))\n",
    "        self.last_observation = observation\n",
    "\n",
    "        return self.ACTIONS[action.item()]\n",
    "\n",
    "    def init_game(self, observation):\n",
    "        self.memory = []\n",
    "        self.rewards = []\n",
    "        self.total_reward = 0\n",
    "\n",
    "    def update(self, observation, reward, terminated, truncated, info, status):\n",
    "        self.total_reward += reward\n",
    "        self.rewards.append(reward)\n",
    "        if terminated:\n",
    "            self.games += 1\n",
    "            if self.mean_reward is None:\n",
    "                self.mean_reward = self.total_reward\n",
    "            else:\n",
    "                self.mean_reward = self.mean_reward * 0.95 + self.total_reward * (1.0 - 0.95)\n",
    "\n",
    "            # calculate discounted reward and make it normal distributed\n",
    "            discounted = []\n",
    "            R = 0\n",
    "            for r in self.rewards[::-1]:\n",
    "                R = r + self.gamma * R\n",
    "                discounted.insert(0, R)\n",
    "            discounted = torch.tensor(discounted)\n",
    "            # discounted = (discounted - discounted.mean()) / (discounted.std() + self.eps)\n",
    "\n",
    "            policy_losses = []\n",
    "            value_losses = []\n",
    "            for mem, discounted_reward in zip(self.memory, discounted):\n",
    "                advantage = discounted_reward - mem.value.item()\n",
    "                policy_losses.append(-(mem.log_prob * advantage))\n",
    "\n",
    "                value_losses.append(F.smooth_l1_loss(mem.value, discounted_reward.unsqueeze(0)))\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if self.games % 1000 == 0:\n",
    "                self.save(f\"model_{self.games}.pt\")\n",
    "\n",
    "    def load(self, PATH):\n",
    "        checkpoint = torch.load(PATH)\n",
    "        self.net.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.games = checkpoint['games']\n",
    "        self.mean_reward = checkpoint['mean_reward']\n",
    "\n",
    "    def save(self, PATH):\n",
    "        torch.save({\n",
    "            'games': self.games,\n",
    "            'model_state_dict': self.net.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'mean_reward': self.mean_reward}, PATH)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Studium_local\\Chess Chatbot venv\\myenv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:31: UserWarning: \u001B[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (8, 8)\u001B[0m\n",
      "  logger.warn(\n",
      "H:\\Studium_local\\Chess Chatbot venv\\myenv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:174: UserWarning: \u001B[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001B[0m\n",
      "  logger.warn(\n",
      "H:\\Studium_local\\Chess Chatbot venv\\myenv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:187: UserWarning: \u001B[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001B[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ChessEnvV2.reset() got an unexpected keyword argument 'seed'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m policy \u001B[38;5;241m=\u001B[39m ACPolicy()\n\u001B[1;32m----> 2\u001B[0m \u001B[43mplay_game\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpolicy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepisodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdo_render\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[9], line 8\u001B[0m, in \u001B[0;36mplay_game\u001B[1;34m(policy, episodes, do_render, seed)\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m      7\u001B[0m     env \u001B[38;5;241m=\u001B[39m gym\u001B[38;5;241m.\u001B[39mmake(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mChessVsSelf-v2\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 8\u001B[0m observation, info \u001B[38;5;241m=\u001B[39m \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseed\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m policy\u001B[38;5;241m.\u001B[39minit_game(observation)\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m do_render:\n",
      "File \u001B[1;32mH:\\Studium_local\\Chess Chatbot venv\\myenv\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:42\u001B[0m, in \u001B[0;36mOrderEnforcing.reset\u001B[1;34m(self, **kwargs)\u001B[0m\n\u001B[0;32m     40\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Resets the environment with `kwargs`.\"\"\"\u001B[39;00m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_reset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m---> 42\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mreset(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mH:\\Studium_local\\Chess Chatbot venv\\myenv\\lib\\site-packages\\gym\\wrappers\\env_checker.py:45\u001B[0m, in \u001B[0;36mPassiveEnvChecker.reset\u001B[1;34m(self, **kwargs)\u001B[0m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchecked_reset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[0;32m     44\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchecked_reset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m---> 45\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m env_reset_passive_checker(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv\u001B[38;5;241m.\u001B[39mreset(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mH:\\Studium_local\\Chess Chatbot venv\\myenv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:192\u001B[0m, in \u001B[0;36menv_reset_passive_checker\u001B[1;34m(env, **kwargs)\u001B[0m\n\u001B[0;32m    187\u001B[0m     logger\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    188\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFuture gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    189\u001B[0m     )\n\u001B[0;32m    191\u001B[0m \u001B[38;5;66;03m# Checks the result of env.reset with kwargs\u001B[39;00m\n\u001B[1;32m--> 192\u001B[0m result \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mreset(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    194\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(result, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m    195\u001B[0m     logger\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    196\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(result)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    197\u001B[0m     )\n",
      "\u001B[1;31mTypeError\u001B[0m: ChessEnvV2.reset() got an unexpected keyword argument 'seed'"
     ]
    }
   ],
   "source": [
    "policy = ACPolicy()\n",
    "play_game_gawron(policy, episodes=10, do_render=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
