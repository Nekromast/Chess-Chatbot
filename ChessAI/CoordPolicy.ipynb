{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym_chess import ChessEnvV2\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from tqdm.notebook import tqdm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "outputs": [],
   "source": [
    "class ChessPolicyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ChessPolicyNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(12, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 64)\n",
    "        self.fc4_start = nn.Linear(64, 64)\n",
    "        self.fc4_end = nn.Linear(64, 64*64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x_start = self.fc4_start(x)\n",
    "        x_end = self.fc4_end(x).view(1, 64, 64)\n",
    "\n",
    "        print(x_start)\n",
    "        print(x_end)\n",
    "        return x_start, x_end"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "SavedAction = namedtuple('SavedAction', ['start_prob', 'end_prob'])\n",
    "\n",
    "class ChessAI:\n",
    "    def __init__(self):\n",
    "        self.net = ChessPolicyNet().to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=5e-3)\n",
    "        self.mean_reward = None\n",
    "        self.games = 0\n",
    "        self.gamma = 0.99\n",
    "        self.eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "    def __call__(self, observation):\n",
    "        board = np.array(observation['board'])\n",
    "        one_hot_board = np.zeros((8, 8, 12))\n",
    "        for i in range(8):\n",
    "            for j in range(8):\n",
    "                piece = board[i][j]\n",
    "                if piece != 0:\n",
    "                    if piece > 0:\n",
    "                        one_hot_board[i][j][abs(piece) - 1] = 1\n",
    "                    else:\n",
    "                        one_hot_board[i][j][abs(piece) + 5] = 1\n",
    "        one_hot_board = np.transpose(one_hot_board, (2, 0, 1))\n",
    "        x = torch.from_numpy(one_hot_board).float().unsqueeze(0)\n",
    "        x = x.to(device)\n",
    "        x_start, x_end = self.net(x)\n",
    "\n",
    "        start_probs = F.softmax(x_start, dim=1).squeeze()\n",
    "        end_probs = F.softmax(x_end, dim=1).squeeze()\n",
    "\n",
    "\n",
    "        m_start = Categorical(start_probs)\n",
    "        start_tensor = m_start.sample(sample_shape=torch.Size([]))\n",
    "        start_item = start_tensor.item()\n",
    "        start_pos = (start_item // 8, start_item % 8)\n",
    "\n",
    "        m_end = Categorical(end_probs[start_item])\n",
    "        end_tensor = m_end.sample(sample_shape=torch.Size([]))\n",
    "        end_item = end_tensor.item()\n",
    "        end_pos = (end_item // 8, end_item % 8)\n",
    "\n",
    "        self.memory.append(SavedAction(m_start.log_prob(start_tensor), m_end.log_prob(end_tensor)))\n",
    "        return start_pos, end_pos\n",
    "\n",
    "    def init_game(self, observation, possible_moves):\n",
    "        self.memory = []\n",
    "        self.rewards = []\n",
    "        self.total_reward = 0\n",
    "\n",
    "    def update(self, observation, reward, terminated, truncated, info, status):\n",
    "        self.total_reward += reward\n",
    "        self.rewards.append(reward)\n",
    "        if terminated:\n",
    "            self.games += 1\n",
    "            if self.mean_reward is None:\n",
    "                self.mean_reward = self.total_reward\n",
    "            else:\n",
    "                self.mean_reward = self.mean_reward * 0.95 + self.total_reward * (1.0 - 0.95)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            # calculate discounted reward and make it normal distributed\n",
    "            discounted = []\n",
    "            R = 0\n",
    "            for r in self.rewards[::-1]:\n",
    "                R = r + self.gamma * R\n",
    "                discounted.insert(0, R)\n",
    "            discounted = torch.Tensor(discounted)\n",
    "            discounted = (discounted - discounted.mean()) / (discounted.std() + self.eps)\n",
    "\n",
    "            policy_losses = []\n",
    "            for mem, discounted_reward in zip(self.memory, discounted):\n",
    "                start_prob, end_prob = mem\n",
    "                policy_losses.append(-((start_prob + end_prob) * discounted_reward))\n",
    "\n",
    "            loss = torch.stack(policy_losses).sum()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if self.games % 2000 == 0:\n",
    "                self.save(f\"models/model_{self.games}.pt\")\n",
    "\n",
    "    def load(self, PATH):\n",
    "        checkpoint = torch.load(PATH)\n",
    "        self.net.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.games = checkpoint['games']\n",
    "        self.mean_reward = checkpoint['mean_reward']\n",
    "\n",
    "    def save(self, PATH):\n",
    "        torch.save({\n",
    "            'games': self.games,\n",
    "            'model_state_dict': self.net.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'mean_reward': self.mean_reward}, PATH)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "outputs": [],
   "source": [
    "from IPython.core.display_functions import clear_output\n",
    "\n",
    "\n",
    "# Play Game Example etwas mit Gawron's Code gemerged\n",
    "def play_game_AI_against_random(episodes=2, steps=50, modelpath=None):\n",
    "    env = gym.make(\"ChessVsRandomBot-v2\", log=False)\n",
    "    env.moves_max = steps\n",
    "    total_rewards = 0\n",
    "    average_rewards = 0\n",
    "    steps_needed = 0\n",
    "    did_legal_move = 0\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    #policy = ACPolicy()\n",
    "    policy = ChessAI()\n",
    "    if modelpath is not None:\n",
    "        policy.load(modelpath)\n",
    "    policy.init_game(observation, env.possible_moves)\n",
    "\n",
    "    for i in range(episodes):\n",
    "        #print(\"\\n\", \"=\" * 10, \"NEW GAME\", \"=\" * 10)\n",
    "        #env.render()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for j in range(steps):\n",
    "\n",
    "            #moves = env.possible_moves\n",
    "            #move = random.choice(moves)\n",
    "            move = policy(observation)\n",
    "            print(move)\n",
    "            print(\"\\n\\n\\n\")\n",
    "            for move in env.possible_moves:\n",
    "                print(move)\n",
    "            action = env.move_to_action(move)\n",
    "            if move in env.possible_moves:\n",
    "                did_legal_move += 1\n",
    "            #Clear prints\n",
    "            #for i in range(20):\n",
    "            #   clear_output(wait=True)\n",
    "\n",
    "            # Eigene Aktion an das Spiel weitergeben\n",
    "            observation, step_reward, done, info = env.step(action)\n",
    "            episode_reward += step_reward\n",
    "            policy.update(observation, step_reward, done, False, None, None)\n",
    "\n",
    "            if done:\n",
    "                env.render()\n",
    "                print(\">\" * 5, \"GAME\", i, \"REWARD:\", episode_reward)\n",
    "                #steps_needed = j\n",
    "                break\n",
    "\n",
    "        if not done:\n",
    "            #print(\"Not done, updating policy\")\n",
    "            policy.update(observation, 0, True, False, None, None)\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "\n",
    "        # Episode zu Ende\n",
    "        observation = env.reset()\n",
    "        policy.init_game(observation, env.possible_moves)\n",
    "\n",
    "        total_rewards += episode_reward\n",
    "        average_rewards = 0.05 * episode_reward + (1 - 0.05) * average_rewards\n",
    "\n",
    "    #policy.save(f\"models/model_{policy.games}.pt\")\n",
    "    print(\"\\n\")\n",
    "    print(\"#\" * 40)\n",
    "    print(\"#\" * 40)\n",
    "    print(\"#\" * 40)\n",
    "    print(\"\\nAVERAGE SCORE: \", average_rewards)\n",
    "    print(\"\\nTOTAL REWARD: \", total_rewards)\n",
    "    print(\"\\nTOTAL LEGAL MOVES: \", did_legal_move)\n",
    "    print(\"\\nLEGAL MOVES AVERAGE: \", did_legal_move / episodes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-15.4474, -25.6004,  16.7986,   0.9408,  -9.4818,   1.4616,  -0.6489,\n",
      "           8.2739,   4.2529, -10.9186, -22.5281,  -8.9942,   8.2626,  -0.9930,\n",
      "          -5.1427,   0.6675, -15.3259,  13.1084,  -1.9304,   5.6925, -16.6272,\n",
      "           2.5188, -18.1875, -25.2354,   1.9778, -13.8683,  10.4875,  -9.1018,\n",
      "           9.5307, -14.7131,  -8.7599,  16.6326,  -0.6079,  -9.0558, -14.4124,\n",
      "           6.6649,   2.2202,  -2.0030,  -1.7415, -15.6497,  -4.8630,  12.6843,\n",
      "         -19.8256, -14.6819, -17.8932,  23.2788,  11.5280,  11.0472,  -8.3830,\n",
      "          -6.5264, -11.6927, -10.7742,  11.2424,  16.5358,   3.6741,  -4.6306,\n",
      "           1.3768,   1.5987,  63.1055,   6.0451,  13.0604, -24.9358,  -2.8851,\n",
      "           0.9793]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[[ -2.0576,   8.7340,  33.7056,  ...,  -2.0465,  -6.9271,   7.6680],\n",
      "         [  8.7443,   4.6823,  25.1787,  ...,  -3.4114, -12.9048,  -4.7832],\n",
      "         [-27.0721,   2.4943,  -1.2878,  ...,  -9.5713,  13.3593,  -2.6732],\n",
      "         ...,\n",
      "         [ 11.5038,  15.9923,  29.1017,  ...,   3.2696,   6.3483,  40.4420],\n",
      "         [  5.0055, -11.5528,  17.4203,  ...,  -7.4057,   4.4022, -20.0788],\n",
      "         [ -6.5583,  14.2164,   2.2137,  ...,   6.9513, -26.1881,  10.3132]]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "((7, 2), (6, 3))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "((6, 0), (5, 0))\n",
      "((6, 0), (4, 0))\n",
      "((6, 1), (5, 1))\n",
      "((6, 1), (4, 1))\n",
      "((6, 2), (5, 2))\n",
      "((6, 2), (4, 2))\n",
      "((6, 3), (5, 3))\n",
      "((6, 3), (4, 3))\n",
      "((6, 4), (5, 4))\n",
      "((6, 4), (4, 4))\n",
      "((6, 5), (5, 5))\n",
      "((6, 5), (4, 5))\n",
      "((6, 6), (5, 6))\n",
      "((6, 6), (4, 6))\n",
      "((6, 7), (5, 7))\n",
      "((6, 7), (4, 7))\n",
      "((7, 1), (5, 0))\n",
      "((7, 1), (5, 2))\n",
      "((7, 6), (5, 5))\n",
      "((7, 6), (5, 7))\n",
      "tensor([[-15.4474, -25.6004,  16.7986,   0.9408,  -9.4818,   1.4616,  -0.6489,\n",
      "           8.2739,   4.2529, -10.9186, -22.5281,  -8.9942,   8.2626,  -0.9930,\n",
      "          -5.1427,   0.6675, -15.3259,  13.1084,  -1.9304,   5.6925, -16.6272,\n",
      "           2.5188, -18.1875, -25.2354,   1.9778, -13.8683,  10.4875,  -9.1018,\n",
      "           9.5307, -14.7131,  -8.7599,  16.6326,  -0.6079,  -9.0558, -14.4124,\n",
      "           6.6649,   2.2202,  -2.0030,  -1.7415, -15.6497,  -4.8630,  12.6843,\n",
      "         -19.8256, -14.6819, -17.8932,  23.2788,  11.5280,  11.0472,  -8.3830,\n",
      "          -6.5264, -11.6927, -10.7742,  11.2424,  16.5358,   3.6741,  -4.6306,\n",
      "           1.3768,   1.5987,  63.1055,   6.0451,  13.0604, -24.9358,  -2.8851,\n",
      "           0.9793]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[[ -2.0576,   8.7340,  33.7056,  ...,  -2.0465,  -6.9271,   7.6680],\n",
      "         [  8.7443,   4.6823,  25.1787,  ...,  -3.4114, -12.9048,  -4.7832],\n",
      "         [-27.0721,   2.4943,  -1.2878,  ...,  -9.5713,  13.3593,  -2.6732],\n",
      "         ...,\n",
      "         [ 11.5038,  15.9923,  29.1017,  ...,   3.2696,   6.3483,  40.4420],\n",
      "         [  5.0055, -11.5528,  17.4203,  ...,  -7.4057,   4.4022, -20.0788],\n",
      "         [ -6.5583,  14.2164,   2.2137,  ...,   6.9513, -26.1881,  10.3132]]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "((7, 2), (6, 3))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "((5, 7), (3, 6))\n",
      "((5, 7), (7, 6))\n",
      "((5, 7), (4, 5))\n",
      "((6, 0), (5, 0))\n",
      "((6, 0), (4, 0))\n",
      "((6, 1), (5, 1))\n",
      "((6, 1), (4, 1))\n",
      "((6, 2), (5, 2))\n",
      "((6, 2), (4, 2))\n",
      "((6, 3), (5, 3))\n",
      "((6, 3), (4, 3))\n",
      "((6, 4), (5, 4))\n",
      "((6, 4), (4, 4))\n",
      "((6, 5), (5, 5))\n",
      "((6, 5), (4, 5))\n",
      "((6, 6), (5, 6))\n",
      "((6, 6), (4, 6))\n",
      "((6, 7), (4, 7))\n",
      "((7, 1), (5, 0))\n",
      "((7, 1), (5, 2))\n",
      "((7, 7), (7, 6))\n",
      "tensor([[-15.4474, -25.6004,  16.7986,   0.9408,  -9.4818,   1.4616,  -0.6489,\n",
      "           8.2739,   4.2529, -10.9186, -22.5281,  -8.9942,   8.2626,  -0.9930,\n",
      "          -5.1427,   0.6675, -15.3259,  13.1084,  -1.9304,   5.6925, -16.6272,\n",
      "           2.5188, -18.1875, -25.2354,   1.9778, -13.8683,  10.4875,  -9.1018,\n",
      "           9.5307, -14.7131,  -8.7599,  16.6326,  -0.6079,  -9.0558, -14.4124,\n",
      "           6.6649,   2.2202,  -2.0030,  -1.7415, -15.6497,  -4.8630,  12.6843,\n",
      "         -19.8256, -14.6819, -17.8932,  23.2788,  11.5280,  11.0472,  -8.3830,\n",
      "          -6.5264, -11.6927, -10.7742,  11.2424,  16.5358,   3.6741,  -4.6306,\n",
      "           1.3768,   1.5987,  63.1055,   6.0451,  13.0604, -24.9358,  -2.8851,\n",
      "           0.9793]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[[ -2.0576,   8.7340,  33.7056,  ...,  -2.0465,  -6.9271,   7.6680],\n",
      "         [  8.7443,   4.6823,  25.1787,  ...,  -3.4114, -12.9048,  -4.7832],\n",
      "         [-27.0721,   2.4943,  -1.2878,  ...,  -9.5713,  13.3593,  -2.6732],\n",
      "         ...,\n",
      "         [ 11.5038,  15.9923,  29.1017,  ...,   3.2696,   6.3483,  40.4420],\n",
      "         [  5.0055, -11.5528,  17.4203,  ...,  -7.4057,   4.4022, -20.0788],\n",
      "         [ -6.5583,  14.2164,   2.2137,  ...,   6.9513, -26.1881,  10.3132]]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "((7, 2), (0, 4))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "((5, 7), (3, 6))\n",
      "((5, 7), (4, 5))\n",
      "((6, 0), (5, 0))\n",
      "((6, 0), (4, 0))\n",
      "((6, 1), (5, 1))\n",
      "((6, 1), (4, 1))\n",
      "((6, 2), (5, 2))\n",
      "((6, 2), (4, 2))\n",
      "((6, 3), (5, 3))\n",
      "((6, 3), (4, 3))\n",
      "((6, 4), (5, 4))\n",
      "((6, 4), (4, 4))\n",
      "((6, 5), (5, 5))\n",
      "((6, 5), (4, 5))\n",
      "((6, 6), (5, 6))\n",
      "((6, 6), (4, 6))\n",
      "((6, 7), (4, 7))\n",
      "((7, 1), (5, 0))\n",
      "((7, 1), (5, 2))\n",
      "((7, 6), (7, 7))\n",
      "tensor([[-15.4474, -25.6004,  16.7986,   0.9408,  -9.4818,   1.4616,  -0.6489,\n",
      "           8.2739,   4.2529, -10.9186, -22.5281,  -8.9942,   8.2626,  -0.9930,\n",
      "          -5.1427,   0.6675, -15.3259,  13.1084,  -1.9304,   5.6925, -16.6272,\n",
      "           2.5188, -18.1875, -25.2354,   1.9778, -13.8683,  10.4875,  -9.1018,\n",
      "           9.5307, -14.7131,  -8.7599,  16.6326,  -0.6079,  -9.0558, -14.4124,\n",
      "           6.6649,   2.2202,  -2.0030,  -1.7415, -15.6497,  -4.8630,  12.6843,\n",
      "         -19.8256, -14.6819, -17.8932,  23.2788,  11.5280,  11.0472,  -8.3830,\n",
      "          -6.5264, -11.6927, -10.7742,  11.2424,  16.5358,   3.6741,  -4.6306,\n",
      "           1.3768,   1.5987,  63.1055,   6.0451,  13.0604, -24.9358,  -2.8851,\n",
      "           0.9793]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[[ -2.0576,   8.7340,  33.7056,  ...,  -2.0465,  -6.9271,   7.6680],\n",
      "         [  8.7443,   4.6823,  25.1787,  ...,  -3.4114, -12.9048,  -4.7832],\n",
      "         [-27.0721,   2.4943,  -1.2878,  ...,  -9.5713,  13.3593,  -2.6732],\n",
      "         ...,\n",
      "         [ 11.5038,  15.9923,  29.1017,  ...,   3.2696,   6.3483,  40.4420],\n",
      "         [  5.0055, -11.5528,  17.4203,  ...,  -7.4057,   4.4022, -20.0788],\n",
      "         [ -6.5583,  14.2164,   2.2137,  ...,   6.9513, -26.1881,  10.3132]]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "((7, 2), (6, 3))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "((5, 7), (3, 6))\n",
      "((5, 7), (7, 6))\n",
      "((5, 7), (4, 5))\n",
      "((6, 0), (5, 0))\n",
      "((6, 0), (4, 0))\n",
      "((6, 1), (5, 1))\n",
      "((6, 1), (4, 1))\n",
      "((6, 2), (5, 2))\n",
      "((6, 2), (4, 2))\n",
      "((6, 3), (5, 3))\n",
      "((6, 3), (4, 3))\n",
      "((6, 4), (5, 4))\n",
      "((6, 4), (4, 4))\n",
      "((6, 5), (5, 5))\n",
      "((6, 5), (4, 5))\n",
      "((6, 6), (5, 6))\n",
      "((6, 6), (4, 6))\n",
      "((6, 7), (4, 7))\n",
      "((7, 1), (5, 0))\n",
      "((7, 1), (5, 2))\n",
      "((7, 7), (7, 6))\n",
      "tensor([[-15.4474, -25.6004,  16.7986,   0.9408,  -9.4818,   1.4616,  -0.6489,\n",
      "           8.2739,   4.2529, -10.9186, -22.5281,  -8.9942,   8.2626,  -0.9930,\n",
      "          -5.1427,   0.6675, -15.3259,  13.1084,  -1.9304,   5.6925, -16.6272,\n",
      "           2.5188, -18.1875, -25.2354,   1.9778, -13.8683,  10.4875,  -9.1018,\n",
      "           9.5307, -14.7131,  -8.7599,  16.6326,  -0.6079,  -9.0558, -14.4124,\n",
      "           6.6649,   2.2202,  -2.0030,  -1.7415, -15.6497,  -4.8630,  12.6843,\n",
      "         -19.8256, -14.6819, -17.8932,  23.2788,  11.5280,  11.0472,  -8.3830,\n",
      "          -6.5264, -11.6927, -10.7742,  11.2424,  16.5358,   3.6741,  -4.6306,\n",
      "           1.3768,   1.5987,  63.1055,   6.0451,  13.0604, -24.9358,  -2.8851,\n",
      "           0.9793]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "tensor([[[ -2.0576,   8.7340,  33.7056,  ...,  -2.0465,  -6.9271,   7.6680],\n",
      "         [  8.7443,   4.6823,  25.1787,  ...,  -3.4114, -12.9048,  -4.7832],\n",
      "         [-27.0721,   2.4943,  -1.2878,  ...,  -9.5713,  13.3593,  -2.6732],\n",
      "         ...,\n",
      "         [ 11.5038,  15.9923,  29.1017,  ...,   3.2696,   6.3483,  40.4420],\n",
      "         [  5.0055, -11.5528,  17.4203,  ...,  -7.4057,   4.4022, -20.0788],\n",
      "         [ -6.5583,  14.2164,   2.2137,  ...,   6.9513, -26.1881,  10.3132]]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "((7, 2), (4, 1))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "((5, 7), (3, 6))\n",
      "((5, 7), (4, 5))\n",
      "((6, 0), (5, 0))\n",
      "((6, 0), (4, 0))\n",
      "((6, 1), (5, 1))\n",
      "((6, 1), (4, 1))\n",
      "((6, 2), (5, 2))\n",
      "((6, 2), (4, 2))\n",
      "((6, 3), (5, 3))\n",
      "((6, 3), (4, 3))\n",
      "((6, 4), (5, 4))\n",
      "((6, 4), (4, 4))\n",
      "((6, 5), (5, 5))\n",
      "((6, 5), (4, 5))\n",
      "((6, 6), (5, 6))\n",
      "((6, 6), (4, 6))\n",
      "((6, 7), (4, 7))\n",
      "((7, 1), (5, 0))\n",
      "((7, 1), (5, 2))\n",
      "((7, 6), (7, 7))\n",
      "\n",
      "\n",
      "########################################\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "AVERAGE SCORE:  -2.5\n",
      "\n",
      "TOTAL REWARD:  -50\n",
      "\n",
      "TOTAL LEGAL MOVES:  5\n",
      "\n",
      "LEGAL MOVES AVERAGE:  5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Studium_local\\Chess Chatbot github\\venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:219: DeprecationWarning: \u001B[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001B[0m\n",
      "  logger.deprecation(\n",
      "H:\\Studium_local\\Chess Chatbot github\\venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:225: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n",
      "H:\\Studium_local\\Chess Chatbot github\\venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:137: UserWarning: \u001B[33mWARN: The obs returned by the `step()` method was expecting a numpy array, actual type: <class 'dict'>\u001B[0m\n",
      "  logger.warn(\n",
      "H:\\Studium_local\\Chess Chatbot github\\venv\\lib\\site-packages\\gym\\spaces\\box.py:227: UserWarning: \u001B[33mWARN: Casting input x to numpy array.\u001B[0m\n",
      "  logger.warn(\"Casting input x to numpy array.\")\n",
      "H:\\Studium_local\\Chess Chatbot github\\venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:165: UserWarning: \u001B[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001B[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    }
   ],
   "source": [
    "play_game_AI_against_random(1, 5, \"models/model_1000.pt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
