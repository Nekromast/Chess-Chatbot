{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 328,
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym_chess import ChessEnvV2\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from tqdm.notebook import tqdm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "outputs": [],
   "source": [
    "class ChessPolicyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ChessPolicyNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(12, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 64)\n",
    "        self.fc4_start = nn.Linear(64, 64)\n",
    "        self.fc4_end = nn.Linear(64, 64*64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x_start = self.fc4_start(x)\n",
    "        x_end = self.fc4_end(x).view(1, 64, 64)\n",
    "\n",
    "        start_probs = F.softmax(x_start, dim=1).squeeze().detach().numpy()\n",
    "        end_probs = F.softmax(x_end, dim=1).squeeze().detach().numpy()\n",
    "\n",
    "        return start_probs, end_probs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n",
    "\n",
    "class ChessAI:\n",
    "    def __init__(self):\n",
    "        self.net = ChessPolicyNet()\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=5e-3)\n",
    "        self.mean_reward = None\n",
    "        self.games = 0\n",
    "        self.gamma = 0.99\n",
    "        self.eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "    def __call__(self, observation):\n",
    "        board = np.array(observation['board'])\n",
    "        one_hot_board = np.zeros((8, 8, 12))\n",
    "        for i in range(8):\n",
    "            for j in range(8):\n",
    "                piece = board[i][j]\n",
    "                if piece != 0:\n",
    "                    if piece > 0:\n",
    "                        one_hot_board[i][j][abs(piece) - 1] = 1\n",
    "                    else:\n",
    "                        one_hot_board[i][j][abs(piece) + 5] = 1\n",
    "        one_hot_board = np.transpose(one_hot_board, (2, 0, 1))\n",
    "        x = torch.from_numpy(one_hot_board).float().unsqueeze(0)\n",
    "        start_probs, end_probs = self.net(x)\n",
    "\n",
    "        m_start = Categorical(torch.from_numpy(start_probs))\n",
    "        start_item = m_start.sample(sample_shape=torch.Size([])).item()\n",
    "        start_pos = (start_item // 8, start_item % 8)\n",
    "\n",
    "        m_end = Categorical(torch.from_numpy(end_probs[start_item]))\n",
    "        end_item = m_end.sample(sample_shape=torch.Size([])).item()\n",
    "        end_pos = (end_item // 8, end_item % 8)\n",
    "\n",
    "        return start_pos, end_pos\n",
    "\n",
    "    def init_game(self, observation, possible_moves):\n",
    "        self.memory = []\n",
    "        self.rewards = []\n",
    "        self.total_reward = 0\n",
    "\n",
    "    def update(self, observation, reward, terminated, truncated, info, status):\n",
    "        self.total_reward += reward\n",
    "        self.rewards.append(reward)\n",
    "        if terminated:\n",
    "            self.games += 1\n",
    "            if self.mean_reward is None:\n",
    "                self.mean_reward = self.total_reward\n",
    "            else:\n",
    "                self.mean_reward = self.mean_reward * 0.95 + self.total_reward * (1.0 - 0.95)\n",
    "\n",
    "            # calculate discounted reward and make it normal distributed\n",
    "            discounted = []\n",
    "            R = 0\n",
    "            for r in self.rewards[::-1]:\n",
    "                R = r + self.gamma * R\n",
    "                discounted.insert(0, R)\n",
    "            discounted = torch.tensor(discounted)\n",
    "            # discounted = (discounted - discounted.mean()) / (discounted.std() + self.eps)\n",
    "\n",
    "            policy_losses = []\n",
    "            value_losses = []\n",
    "            for mem, discounted_reward in zip(self.memory, discounted):\n",
    "                advantage = discounted_reward - mem.value.item()\n",
    "                policy_losses.append(-(mem.log_prob * advantage))\n",
    "\n",
    "                value_losses.append(F.smooth_l1_loss(mem.value, discounted_reward.unsqueeze(0)))\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if self.games % 1000 == 0:\n",
    "                self.save(f\"model_{self.games}.pt\")\n",
    "\n",
    "    def load(self, PATH):\n",
    "        checkpoint = torch.load(PATH)\n",
    "        self.net.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.games = checkpoint['games']\n",
    "        self.mean_reward = checkpoint['mean_reward']\n",
    "\n",
    "    def save(self, PATH):\n",
    "        torch.save({\n",
    "            'games': self.games,\n",
    "            'model_state_dict': self.net.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'mean_reward': self.mean_reward}, PATH)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "outputs": [],
   "source": [
    "from IPython.core.display_functions import clear_output\n",
    "\n",
    "\n",
    "# Play Game Example etwas mit Gawron's Code gemerged\n",
    "def play_game_AI_against_random(episodes=2, steps=50):\n",
    "    env = gym.make(\"ChessVsRandomBot-v2\", log=False)\n",
    "    env.moves_max = steps\n",
    "    total_rewards = 0\n",
    "    average_rewards = 0\n",
    "    steps_needed = 0\n",
    "    did_legal_move = 0\n",
    "    observation = env.reset()\n",
    "    #policy = ACPolicy()\n",
    "    policy = ChessAI()\n",
    "    policy.init_game(observation, env.possible_moves)\n",
    "\n",
    "    for i in range(episodes):\n",
    "        print(\"\\n\", \"=\" * 10, \"NEW GAME\", \"=\" * 10)\n",
    "        #env.render()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for j in range(steps):\n",
    "\n",
    "            #moves = env.possible_moves\n",
    "            #move = random.choice(moves)\n",
    "            move = policy(observation)\n",
    "            action = env.move_to_action(move)\n",
    "            if move in env.possible_moves:\n",
    "                did_legal_move += 1\n",
    "            #Clear prints\n",
    "            #for i in range(20):\n",
    "            #   clear_output(wait=True)\n",
    "\n",
    "            # Eigene Aktion an das Spiel weitergeben\n",
    "            observation, step_reward, done, info = env.step(action)\n",
    "            episode_reward += step_reward\n",
    "            policy.update(observation, step_reward, done, False, None, None)\n",
    "\n",
    "            if done:\n",
    "                env.render()\n",
    "                print(\">\" * 5, \"GAME\", i, \"REWARD:\", episode_reward)\n",
    "                #steps_needed = j\n",
    "                break\n",
    "\n",
    "        # Episode zu Ende\n",
    "        observation = env.reset()\n",
    "        policy.init_game(observation, env.possible_moves)\n",
    "\n",
    "        total_rewards += episode_reward\n",
    "        average_rewards = 0.05 * episode_reward + (1 - 0.05) * average_rewards\n",
    "        # Kein Reward Threshold angegeben\n",
    "        '''if average_rewards > env.spec.reward_threshold:\n",
    "            print(f\"Du bist zu gut für das Spiel :( \\n \"\n",
    "                  f\"Dein Average Reward beträgt {average_rewards} und du brauchtest nur {steps_needed} Schritte zum Sieg!\")\n",
    "        '''\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"#\" * 40)\n",
    "    print(\"#\" * 40)\n",
    "    print(\"#\" * 40)\n",
    "    print(\"\\nAVERAGE SCORE: \", average_rewards)\n",
    "    print(\"\\nTOTAL REWARD: \", total_rewards)\n",
    "    print(\"\\nLEGAL MOVES AVERAGE: \", did_legal_move / episodes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ========== NEW GAME ==========\n",
      "\n",
      " ========== NEW GAME ==========\n",
      "\n",
      " ========== NEW GAME ==========\n",
      "\n",
      " ========== NEW GAME ==========\n",
      "\n",
      " ========== NEW GAME ==========\n",
      "\n",
      " ========== NEW GAME ==========\n",
      "\n",
      " ========== NEW GAME ==========\n",
      "\n",
      " ========== NEW GAME ==========\n",
      "\n",
      " ========== NEW GAME ==========\n",
      "\n",
      " ========== NEW GAME ==========\n",
      "\n",
      " ========== NEW GAME ==========\n",
      "\n",
      " ========== NEW GAME ==========\n",
      "\n",
      " ========== NEW GAME ==========\n",
      "\n",
      " ========== NEW GAME ==========\n",
      "\n",
      " ========== NEW GAME ==========\n",
      "\n",
      " ========== NEW GAME ==========\n",
      "\n",
      " ========== NEW GAME ==========\n",
      "\n",
      " ========== NEW GAME ==========\n",
      "\n",
      " ========== NEW GAME ==========\n",
      "\n",
      " ========== NEW GAME ==========\n",
      "\n",
      "\n",
      "########################################\n",
      "########################################\n",
      "########################################\n",
      "\n",
      "AVERAGE SCORE:  -641.5140775914574\n",
      "\n",
      "TOTAL REWARD:  -20000\n",
      "\n",
      "LEGAL MOVES AVERAGE:  0.35\n"
     ]
    }
   ],
   "source": [
    "play_game_AI_against_random(20, 100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
