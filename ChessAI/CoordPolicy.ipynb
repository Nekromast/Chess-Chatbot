{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym_chess import ChessEnvV2\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from collections import namedtuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from tqdm.notebook import tqdm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ChessPolicyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ChessPolicyNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(12, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 64)\n",
    "        self.fc4_start = nn.Linear(64, 64)\n",
    "        self.fc4_end = nn.Linear(64, 64 * 64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = self.flatten(x)\n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = F.elu(self.fc2(x))\n",
    "        x = F.elu(self.fc3(x))\n",
    "        x_start = self.fc4_start(x)\n",
    "        x_end = self.fc4_end(x).view(1, 64, 64)\n",
    "\n",
    "        return x_start, x_end"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SavedAction = namedtuple('SavedAction', ['start_prob', 'end_prob'])\n",
    "class ChessAI:\n",
    "    def __init__(self):\n",
    "        self.net = ChessPolicyNet().to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.03)\n",
    "        self.mean_reward = None\n",
    "        self.games = 0\n",
    "        self.gamma = 0.99\n",
    "        self.eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "    def __call__(self, observation, possible_moves):\n",
    "        board = np.array(observation['board'])\n",
    "        one_hot_board = np.zeros((8, 8, 12))\n",
    "        for i in range(8):\n",
    "            for j in range(8):\n",
    "                piece = board[i][j]\n",
    "                if piece != 0:\n",
    "                    if piece > 0:\n",
    "                        one_hot_board[i][j][abs(piece) - 1] = 1\n",
    "                    else:\n",
    "                        one_hot_board[i][j][abs(piece) + 5] = 1\n",
    "        one_hot_board = np.transpose(one_hot_board, (2, 0, 1))\n",
    "        x = torch.from_numpy(one_hot_board).float().unsqueeze(0)\n",
    "        x = x.to(device)\n",
    "        x_start, x_end = self.net(x)\n",
    "\n",
    "        # Get starting and end positions that are legal\n",
    "        legal_starting_moves = torch.zeros((8, 8)).cuda()\n",
    "        legal_moves = torch.zeros((8, 8, 8, 8)).cuda()\n",
    "        for move in possible_moves:\n",
    "            if len(move) == 2:\n",
    "                x, y = move[0]\n",
    "                x2, y2 = move[1]\n",
    "                legal_starting_moves[x][y] = 1\n",
    "                legal_moves[x][y][x2][y2] = 1\n",
    "\n",
    "        # Filter out illegal moves\n",
    "        x_start_2d = x_start[0].view(8, 8)\n",
    "        x_start_2d = legal_starting_moves * x_start_2d\n",
    "        x_end_4d = x_end[0].view(8, 8, 8, 8)\n",
    "        x_end_4d = legal_moves * x_end_4d\n",
    "\n",
    "        # Reshape back into 1D Tensors\n",
    "        x_start = x_start_2d.view(1, 64)\n",
    "        x_end = x_end_4d.view(1, 64, 64)\n",
    "\n",
    "        # Softmax starting positions\n",
    "        non_zero_mask = (x_start[0] != 0)  # boolean mask of non-zero elements\n",
    "        non_zero_values = x_start[0][non_zero_mask]  # extract non-zero values\n",
    "        softmaxed_values = torch.softmax(non_zero_values, dim=0)  # apply softmax to non-zero values\n",
    "        start_result = torch.zeros_like(x_start)  # create a tensor with the same shape as probs, filled with zeros\n",
    "        start_result[0][non_zero_mask] = softmaxed_values  # put softmaxed non-zero values back into result tensor\n",
    "\n",
    "        # Sample starting positions\n",
    "        m_start = Categorical(start_result)\n",
    "        start_tensor = m_start.sample(sample_shape=torch.Size([]))\n",
    "        start_item = start_tensor.item()\n",
    "        start_pos = (start_item // 8, start_item % 8)\n",
    "\n",
    "        # Softmax end positions\n",
    "        non_zero_mask = (x_end[0][start_item] != 0)  # boolean mask of non-zero elements\n",
    "        non_zero_values = x_end[0][start_item][non_zero_mask]  # extract non-zero values\n",
    "        softmaxed_values = torch.softmax(non_zero_values, dim=0)  # apply softmax to non-zero values\n",
    "        end_result = torch.zeros_like(x_end)  # create a tensor with the same shape as probs, filled with zeros\n",
    "        end_result[0][start_item][non_zero_mask] = softmaxed_values  # put softmaxed non-zero values back into result tensor\n",
    "\n",
    "        # Sample end positions\n",
    "        m_end = Categorical(end_result[0][start_item])\n",
    "        end_tensor = m_end.sample(sample_shape=torch.Size([]))\n",
    "        end_item = end_tensor.item()\n",
    "        end_pos = (end_item // 8, end_item % 8)\n",
    "\n",
    "        self.memory.append(SavedAction(m_start.log_prob(start_tensor), m_end.log_prob(end_tensor)))\n",
    "        return start_pos, end_pos\n",
    "\n",
    "    def init_game(self, observation, possible_moves):\n",
    "        self.memory = []\n",
    "        self.rewards = []\n",
    "        self.total_reward = 0\n",
    "\n",
    "    def update(self, observation, reward, terminated, truncated, info, status):\n",
    "        self.total_reward += reward\n",
    "        self.rewards.append(reward)\n",
    "        if terminated:\n",
    "            self.games += 1\n",
    "            if self.mean_reward is None:\n",
    "                self.mean_reward = self.total_reward\n",
    "            else:\n",
    "                self.mean_reward = self.mean_reward * 0.95 + self.total_reward * (1.0 - 0.95)\n",
    "\n",
    "            # calculate discounted reward and make it normal distributed\n",
    "            discounted = []\n",
    "            R = 0\n",
    "            for r in self.rewards[::-1]:\n",
    "                R = r + self.gamma * R\n",
    "                discounted.insert(0, R)\n",
    "            discounted = torch.Tensor(discounted)\n",
    "            discounted = (discounted - discounted.mean()) / (discounted.std() + self.eps)\n",
    "\n",
    "            policy_losses = []\n",
    "            for mem, discounted_reward in zip(self.memory, discounted):\n",
    "                start_prob, end_prob = mem\n",
    "                policy_losses.append(-((start_prob + end_prob) * discounted_reward))\n",
    "            # self.optimizer.zero_grad()\n",
    "            loss = torch.stack(policy_losses).sum()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            if self.games % 100 == 0:\n",
    "                self.save(f\"models/model_games_{self.games}.pt\")\n",
    "\n",
    "    def load(self, PATH):\n",
    "        checkpoint = torch.load(PATH)\n",
    "        self.net.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.games = checkpoint['games']\n",
    "        self.mean_reward = checkpoint['mean_reward']\n",
    "\n",
    "    def save(self, PATH):\n",
    "        torch.save({\n",
    "            'games': self.games,\n",
    "            'model_state_dict': self.net.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'mean_reward': self.mean_reward}, PATH)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from IPython.core.display_functions import clear_output\n",
    "\n",
    "\n",
    "# Play Game Example etwas mit Gawron's Code gemerged\n",
    "def play_game_AI_against_random(episodes=2, steps=50, modelpath=None, render=False, save=False):\n",
    "    env = gym.make(\"ChessVsRandomBot-v2\", log=False)\n",
    "    env.moves_max = steps\n",
    "    total_rewards = 0\n",
    "    average_rewards = 0\n",
    "    observation = env.reset()\n",
    "    policy = ChessAI()\n",
    "    if modelpath is not None:\n",
    "        policy.load(modelpath)\n",
    "    policy.init_game(observation, env.possible_moves)\n",
    "\n",
    "    for i in range(episodes):\n",
    "        if render:\n",
    "            print(\"\\n\", \"=\" * 10, \"NEW GAME\", \"=\" * 10)\n",
    "        episode_reward = 0\n",
    "\n",
    "        for j in range(steps):\n",
    "            move = policy(observation, env.possible_moves)\n",
    "            action = env.move_to_action(move)\n",
    "            if render:\n",
    "                #Clear previous render before new render\n",
    "                for k in range(20):\n",
    "                   clear_output(wait=True)\n",
    "                env.render()\n",
    "\n",
    "            # Eigene Aktion an das Spiel weitergeben\n",
    "            observation, step_reward, done, info = env.step(action)\n",
    "            step_reward = step_reward + 10\n",
    "\n",
    "            episode_reward += step_reward\n",
    "            policy.update(observation, step_reward, done, False, None, None)\n",
    "\n",
    "            if done:\n",
    "                env.render()\n",
    "                print(\">\" * 5, \"GAME\", i, \"REWARD:\", episode_reward)\n",
    "                #steps_needed = j\n",
    "                break\n",
    "\n",
    "        # Episode zu Ende\n",
    "        observation = env.reset()\n",
    "        policy.init_game(observation, env.possible_moves)\n",
    "\n",
    "        total_rewards += episode_reward\n",
    "        average_rewards = 0.05 * episode_reward + (1 - 0.05) * average_rewards\n",
    "\n",
    "    if save:\n",
    "        policy.save(f\"models/model_games_{policy.games}.pt\")\n",
    "    print(\"\\n\")\n",
    "    print(\"#\" * 40)\n",
    "    print(\"#\" * 40)\n",
    "    print(\"#\" * 40)\n",
    "    print(\"\\nAVERAGE SCORE: \", average_rewards)\n",
    "    print(\"TOTAL REWARD: \", total_rewards)\n",
    "    print(\"AVERAGE PER EPISODE\", total_rewards / episodes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "play_game_AI_against_random(episodes=1000, steps=100, modelpath=\"models/model_games_100.pt\", render=False, save=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
